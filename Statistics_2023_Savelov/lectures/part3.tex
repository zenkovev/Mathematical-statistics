\subsection{Условное математическое ожидание}

\begin{note}
    Напоминание из курса теории вероятностей.
\end{note}

\begin{definition}
    Пусть $\xi$ -- случайная величина (вектор) на вероятностном пространстве $(\Omega, \F, P)$. Тогда $\sigma$-алгеброй, порождённой $\xi$, называется объект:
    \[
        \F_\xi = \{\xi^{-1}(B) \colon B \in \B(\R^k)\}
    \]
    То есть $\sigma$-алгебра, порождённая $\xi$ -- это прообразы всех борелевских множеств. Нетрудно показать, что это действительно $\sigma$-алгебра.
\end{definition}

\begin{definition}
    Пусть $\cC \subset \F$, $\cC$ -- $\sigma$-алгебра. Случайная величина (вектор) $\xi$ называется $\cC$-измеримой, если $\F_\xi \subset \cC$. Иными словами, все интересные для $\xi$ множества содержатся в $\sigma$-алгебре $\cC$.
\end{definition}

\begin{definition}
    Пусть $\xi, \eta$ -- случайные величины (векторы) на вероятностном пространстве $(\Omega, \F, P)$. $\xi$ называется $\eta$-измеримой, если $\F_\xi \subset \F_\eta$.
\end{definition}

\begin{proposition}
    Пусть $\xi, \eta$ -- случайные величины (векторы) на вероятностном пространстве $(\Omega, \F, P)$. Следующие утверждения эквивалентны:
    \begin{enumerate}
        \item $\xi$ является $\eta$-измеримой
        \item $\xi = \phi(\eta)$, $\phi$ -- борелевская функция
    \end{enumerate}
\end{proposition}

\begin{proof}~
    \begin{itemize}
        \item[$2 \Ra 1$] Так как $\phi$ -- борелевская функция, то $\forall B \in \B(\R^k) \ \phi^{-1}(B) \in \B(\R^m)$, в таком случае:
        \[
            \forall B \in \B(\R^k) \ \ \xi^{-1}(B) = \eta^{-1}(\phi^{-1}(B)) \in \F_\eta \ \ \Ra \ \ \F_\xi \subset \F_\eta
        \]

        \item[$1 \Ra 2$] Доказательство нетривиально, можно посмотреть в книге А. Н. Ширяева Вероятность, том 1, глава 1, параграф 4, теорема 3.
    \end{itemize}
\end{proof}

\begin{definition}
    Пусть $\xi$ -- некоторая случайная величина на вероятностном пространстве $(\Omega, \F, P)$. Пусть $\mathcal{C} \subset \F$, $\mathcal{C}$ -- тоже $\sigma$-алгебра. Условным математическим ожиданием случайной величины $\xi$ относительно $\sigma$-алгебры $\mathcal{C}$ называется случайная величина $\E(\xi | \mathcal{C})$, удовлетворяющая двум свойствам:
    \begin{enumerate}
        \item Свойство измеримости: $\E(\xi | \mathcal{C})$ является $\mathcal{C}$-измеримой случайной величиной, то есть порождённая ей $\sigma$-алгебра $\F_{\E(\xi | \mathcal{C})} \subset \mathcal{C}$.

        \item Интегральное свойство:
        \[
            \forall A \in \mathcal{C} \ \ \E(\xi I_A) = \E(\E(\xi | \mathcal{C}) I_A)
        \]
        Или в терминах интегралов:
        \[
            \forall A \in \mathcal{C} \ \ \int_A \xi dP = \int_A \E(\xi | \mathcal{C}) dP
        \]
    \end{enumerate}
\end{definition}

\begin{note}
    Грубо говоря, класс интересных для случайной величины $\xi$ множеств сужается на $\sigma$-алгебру $\cC$, при этом свойства случайной величины $\xi$ относительно $\sigma$-алгебры $\cC$ сохраняются. Почему в общем случае $\xi \neq \E(\xi | \mathcal{C})$? Интегральное свойство, конечно, выполнено, но $\xi$ не обязательно $\mathcal{C}$-измерима.
\end{note}

\begin{theorem} (Теорема о существовании и единственности УМО, б/д)
    Если $\E|\xi| < +\infty$, то для любой $\mathcal{C}$ -- под-$\sigma$-алгебры $\F$ -- условное математическое ожидание $\E(\xi | \mathcal{C})$ существует и единственно с точностью до равенства почти наверное.
\end{theorem}

\begin{theorem} (Свойства условного математического ожидания)
    \begin{enumerate}
        \item Пусть $\E|\xi| < +\infty$, $\cC$ -- под-$\sigma$-алгебра в $\F$. Пусть $\xi$ -- $\cC$--измеримая случайная величина. Тогда:
        \[
            \E(\xi | \cC) = \xi
        \]

        \item (Линейность) Пусть $\E|\xi| < +\infty$, $\E|\eta| < +\infty$, $\cC$ -- под-$\sigma$-алгебра в $\F$. Пусть $a, b \in \R$. Тогда:
        \[
            \E(a\xi + b\eta | \cC) = a\E(\xi | \cC) + b\E(\eta | \cC)
        \]

        \item (Формула полной вероятности) Пусть $\E|\xi| < +\infty$, $\cC$ -- под-$\sigma$-алгебра в $\F$. Тогда:
        \[
            \E(\E(\xi | \cC)) = \E\xi
        \]

        \item Пусть $\E|\xi| < +\infty$, $\cC$ -- под-$\sigma$-алгебра в $\F$. Пусть $\xi$ независима с $\cC$, то есть порождённая ей $\sigma$-алгебра $\F_\xi$ независима с $\cC$. Тогда:
        \[
            \E(\xi | \cC) = \E\xi
        \]

        \item (Сохранение отношения порядка) Пусть $\E|\xi| < +\infty$, $\E|\eta| < +\infty$, $\cC$ -- под-$\sigma$-алгебра в $\F$. Если $\xi \leqslant \eta$, то:
        \[
            \E(\xi | \cC) \leqslant \E(\eta | \cC) \text{ почти наверное} 
        \]

        \item Пусть $\E|\xi| < +\infty$, $\cC$ -- под-$\sigma$-алгебра в $\F$. Тогда:
        \[
            |\E(\xi | \cC)| \leqslant \E(|\xi| \ | \ \cC) \text{ почти наверное}
        \]

        \item (Телескопическое свойство) Пусть $\E|\xi| < +\infty$, $\cC_1, \cC_2$ -- под-$\sigma$-алгебры в $\F$. Если $\cC_1 \subset \cC_2$, то:
        \begin{itemize}
            \item[(a)] $\E(\E(\xi | \cC_1) | \cC_2) = \E(\xi | \cC_1)$
            \item[(b)] $\E(\E(\xi | \cC_2) | \cC_1) = \E(\xi | \cC_1)$
        \end{itemize}

        \item Пусть $\E|\xi| < +\infty$, $\E|\xi \eta| < +\infty$, $\cC$ -- под-$\sigma$-алгебра в $\F$. Если $\eta$ -- $\cC$--измеримая случайная величина, то:
        \[
            \E(\xi\eta | \cC) = \eta \E(\xi | \cC)
        \]

        \item (Неравенство Йенсена) Пусть $\phi: \R \to \R$ -- выпуклая вниз борелевская функция, $\E|\xi| < +\infty$, $\E|\phi(\xi)| < +\infty$, $\cC$ -- под-$\sigma$-алгебра в $\F$. Тогда:
        \[
            \E(\phi(\xi) | \cC) \geqslant \phi(\E(\xi | \cC)) \text{ почти наверное}
        \]
    \end{enumerate}
\end{theorem}

\begin{note}
    Докажем первые 7 свойств, доказательство остальных можно посмотреть в конспекте курса по теории вероятностей. Во всех свойствах для всех условных матожиданий выполнена теорема о существовании и единственности УМО, если это где-то неочевидно, то будет пояснено в доказательстве. Стоит отметить, что все равенства из свойств можно заменить на равенство почти наверное, так как в таком случае УМО определено однозначно с точностью до множества нулевой вероятности. Практически все свойства будут доказываться так: есть кандидат в условное матожидание, проверим для него выполнение свойств из определения УМО.
\end{note}

\begin{proof}~
    \begin{enumerate}
        \item Выполнены условия теоремы о существовании и единственности УМО. Проверим свойства из определения УМО.

        Свойство измеримости выполнено по условию теоремы.

        Интегральное свойство очевидно.

        \item Так как $\E|a\xi + b\eta| \leqslant |a|\E|\xi|+|b|\E|\eta| < +\infty$, то для всех УМО выполнены условия теоремы о существовании и единственности УМО. Проверим для $a\E(\xi | \cC) + b\E(\eta | \cC)$ свойства из определения УМО $\E(a\xi + b\eta | \cC)$.

        Свойство измеримости: $\cC$-измерима как линейная комбинация $\cC$-измеримых.

        Интегральное свойство: пусть $A \in \cC$:
        \begin{multline*}
            \E((a\xi + b\eta)I_A) = a\E(\xi I_A) + b\E(\eta I_A) = \text{[инт. св-во]} =
            \\
            = a\E(\E(\xi | \cC) I_A) + b\E(\E(\eta | \cC) I_A) = \E((a\E(\xi | \cC) + b\E(\eta | \cC))I_A)
        \end{multline*}

        \item Для всех (которых одно) УМО выполнены условия теоремы о существовании и единственности УМО. Так как $\cC$ -- $\sigma$-алгебра подмножеств $\Omega$, то $\Omega \in \cC$, поэтому в силу интегрального свойства:
        \[
            \E(\E(\xi | \cC)) = \E(\E(\xi | \cC)I_\Omega) = \E(\xi I_\Omega) = \E\xi
        \]

        \item Выполнены условия теоремы о существовании и единственности УМО. Проверим свойства из определения УМО.

        Свойство измеримости для константы $\E\xi$, конечно, выполнено.

        Интегральное свойство: пусть $A \in \cC$. Тогда так как $\F_{I_A} = \sigma(A) \subset \cC$, то просто по условию теоремы и по определению $I_A$ и $\xi$ независимы. Тогда:
        \[
            \E(\xi I_A) = \text{[нез-сть]} = \E\xi \E I_A = \E((\E\xi) I_A)
        \]

        \item Для всех УМО выполнены условия теоремы о существовании и единственности УМО.
        
        Возьмём произвольное $A \in \cC$. Тогда:
        \begin{multline*}
            \xi \leqslant \eta \Ra \xi I_A \leqslant \eta I_A \Ra \E(\xi I_A) \leqslant \E(\eta I_A) \Ra \text{[инт. св-во УМО]} \Ra
            \\
            \Ra \E(\xi I_A) = \E(\E(\xi | \cC) I_A) \leqslant \E(\E(\eta | \cC) I_A) = \E(\eta I_A)
        \end{multline*}

        Получили, что $\forall A \in \cC \ \ \E(\E(\xi | \cC) I_A) \leqslant \E(\E(\eta | \cC) I_A)$.

        По свойству измеримости УМО $\E(\xi | \cC)$ и $\E(\eta | \cC)$ $\cC$-измеримы, тогда случайная величина $\E(\eta | \cC) - \E(\xi | \cC)$ тоже $\cC$-измерима. Тогда $A := \{\E(\eta | \cC) - \E(\xi | \cC) < 0\} \in \cC$.

        Заметим, что
        \begin{align*}
            & \E((\E(\eta | \cC) - \E(\xi | \cC)) I_A) \geqslant 0 \ (\text{т.к. } A \in \cC)
            \\
            & \E((\E(\eta | \cC) - \E(\xi | \cC)) I_A) \leqslant 0 \ (\text{из опр-я } A)
            \\
            & \Downarrow
            \\
            & \E((\E(\eta | \cC) - \E(\xi | \cC)) I_A) = 0
        \end{align*}

        $(\E(\eta | \cC) - \E(\xi | \cC)) I_A$ -- неотрицательная случайная величина, имеющая нулевое матожидание. Тогда она почти наверное равна нулю, то есть $A$ имеет нулевую вероятность. А это и означает, что $\E(\xi | \cC) \leqslant \E(\eta | \cC)$ почти наверное.

        \item Опять для всех УМО выполнены условия теоремы о существовании и единственности УМО. Так как $\xi \leqslant |\xi|$ и $-\xi \leqslant |\xi|$, то в силу предыдущего свойства:
        \begin{align*}
            & \E(\xi | \cC) \leqslant \E(|\xi| \ | \ \cC) \text{ почти наверное}
            \\
            & -\E(\xi | \cC) \leqslant \E(|\xi| \ | \ \cC) \text{ почти наверное}
            \\
            & \Downarrow
            \\
            & | \E(\xi | \cC) | = \max(\E(\xi | \cC), -\E(\xi | \cC)) \leqslant \E(|\xi| \ | \ \cC) \text{ почти наверное}
        \end{align*}

        \item По третьему свойству (формула полной вероятности) $\E(\E(\xi | \cC_1)) = \E(\E(\xi | \cC_2)) = \E\xi$, поэтому все эти матожидания конечны. Тогда для всех УМО выполнены условия теоремы о существовании и единственности УМО. Осталось проверить равенства.
        \begin{itemize}
            \item[(a)] Так как по свойству измеримости УМО $\E(\xi | \cC_1)$ $\cC_1$-измерима, $\cC_1 \subset \cC_2$, то $\E(\xi | \cC_1)$ $\cC_2$-измерима, после чего всё следует из первого свойства.

            \item[(b)] Покажем, что $\E(\xi | \cC_1)$ удовлетворяет определению УМО $\E(\E(\xi | \cC_2) | \cC_1)$. $\E(\xi | \cC_1)$, конечно, $\cC_1$-измерима, теперь пусть $A \in \cC_1$, проверим интегральное свойство:
            \begin{multline*}
                \E( \E(\xi | \cC_2) I_A ) = \text{[инт. св-во, $A \in \cC_2 \supset \cC_1$]} = \E(\xi I_A) =
                \\
                = \E(\xi I_A) = \text{[инт. св-во, $A \in \cC_1$]} = \E( \E(\xi | \cC_1) I_A )
            \end{multline*}
        \end{itemize}
    \end{enumerate}
\end{proof}

\subsection{Условное распределение}

\begin{note}
    Напоминание из курса теории вероятностей.
\end{note}

\begin{definition}
    $\E(\xi | \eta) := \E(\xi | \F_\eta)$ --- условное математическое ожидание случайной величины $\xi$ относительно случайной величины $\eta$. Здесь $\F_\eta$ -- порождённая $\sigma$-алгебра случайной величины $\eta$.
\end{definition}

\begin{note}
    Свойства условного математического ожидания случайной величины $\xi$ относительно $\sigma$-алгебры $\cC$ очевидным образом переносятся на этот случай.
\end{note}

\begin{definition}
    Условным распределением случайной величины $\xi$ относительно случайной величины $\eta$ называется функция $P(B, y),\ B \in \B(\R),\ y \in \R$, удовлетворяющая трём свойствам:
    \begin{enumerate}
        \item При фиксированном $B$ функция $P(B, y)$ является борелевской функцией от $y$
        \item При фиксированном $y$ функция $P(B, y)$ является вероятностной мерой на $(\R, \B(\R))$
        \item $\forall A, B \in \B(\R) \ \ P(\xi \in B,\ \eta \in A) = \int_A P(B, y) P_\eta(dy)$
    \end{enumerate}
    Обозначение: $P(\xi \in B | \eta = y) := P(B, y)$.
\end{definition}

\begin{theorem} (Существование и единственность условного распределения, б/д)
    При $\E|\xi| < +\infty$ условное распределение $P(\xi \in B | \eta = y)$ существует и единственно, единственность можем утверждать $P_\eta$-почти наверное при каждом фиксированном $B$.
\end{theorem}

\subsection{Оптимальные оценки}

\subsubsection{Достаточные статистики}

\begin{note}
    Пусть имеем несимметричную монетку с распределением $Bern(p)$. Пусть сделали 100 бросков и 53 раза выпала 1. Тогда, чтобы сделать вывод о неизвестном параметре $p$, нам нет необходимости хранить данные о всей выборке, достаточно статистики $\sum_{i=1}^n X_i = 53$.
\end{note}

\begin{definition}
    Пусть $X$ -- выборка из неизвестного распределения $P \in \{P_\theta,\ \theta \in \Theta\}$. Тогда статистика $T(X)$ называется достаточной для параметра $\theta$, если условное распределение $P_\theta(X \in B | T(X) = t)$ не зависит от параметра $\theta$.

    К этому определению можно формально придраться, ведь условное распределение $P(\xi \in B | \eta = y)$ единственно, при $\E\xi < \infty$, лишь $P_\eta$-п.н. Формальный ответ такой: существует вариант условного распределения, который не зависит от $\theta$.
\end{definition}

\begin{note}
    Если статистики $S$ и $T$ находятся во взаимно однозначном соответствии и $T$ достаточная, то $S$ тоже достаточная.

    При всей очевидности утверждения я затрудняюсь его аккуратно доказать.
\end{note}

\begin{note}
    Достаточная статистика всегда существует, так как вся выборка $X$ является достаточной статистикой.

    Это уже нетрудно доказать на основе определения.
\end{note}

\begin{theorem} (Нейман, Фишер)
    Пусть $\{P_\theta,\ \theta \in \Theta\}$ -- доминируемое семейство распределений, т.е. есть обобщённая плотность по мере $\mu$. Тогда следующие утверждения эквивалентны:
    \begin{enumerate}
        \item Статистика $T(X)$ является достаточной для параметра $\theta$
        \item Функция правдоподобия $f_\theta(X)$ представима в виде
        \[
            f_\theta(X) = \psi(T(X), \theta) \cdot h(X)
        \]
        
        Здесь $\psi, h$ -- неотрицательные функции, $\psi$ измерима по $T$, $h$ измерима по $X$. Так как неотрицательность и измеримость -- часть требований к плотности, то по сути эти условия отвечают за то, что разложение плотности адекватно.

        И так как плотность тоже определена неоднозначно, опять, существует вариант плотности, который представим в таком виде.
    \end{enumerate}
\end{theorem}

\begin{note}
    Разложение функции правдоподобия из теоремы неоднозначно, например, $f_\theta(X) = \psi h = \frac{\psi}{5} (5h)$.
\end{note}

\begin{proof}
    Для простоты проведём доказательство в дискретном случае.

    \begin{itemize}
        \item[$2 \Ra 1$] Пусть $X$ -- выборка, $x$ -- конкретное значение той же размерности. Тогда распишем условное распределение, достаточно проверить его независимость от $\theta$ в конкретной точке $x$:
        \[
            P_\theta(X = x | T(X) = t) = \frac{P_\theta(X = x, T(X) = t)}{P_\theta(T(X) = t)} = \System{
                & 0,\ T(X) \neq t,
                \\
                & \frac{P_\theta(X = x)}{P_\theta(T(X) = t)},\ \text{иначе}
            }
        \]

        Подробнее распишем нижний случай, то есть $T(X) = t$:
        \begin{multline*}
            P_\theta(X = x | T(X) = t) = \frac{P_\theta(X = x)}{P_\theta(T(X) = t)} = \frac{P_\theta(X = x)}{\sum\limits_{y \colon T(y) = t} P_\theta(X = y)} = \frac{f_\theta(x)}{\sum\limits_{y \colon T(y) = t} f_\theta(y)} =
            \\
            = \frac{\psi(T(x), \theta) \cdot h(x)}{\sum\limits_{y \colon T(y) = t} \psi(T(y), \theta) \cdot h(y)} = \frac{\psi(t, \theta) \cdot h(x)}{\sum\limits_{y \colon T(y) = t} \psi(t, \theta) \cdot h(y)} = \frac{h(x)}{\sum\limits_{y \colon T(y) = t} h(y)}
        \end{multline*}

        Таким образом, действительно, получили независимость от $\theta$:
        \[
            P_\theta(X = x | T(X) = t) = \System{
                & 0,\ T(X) \neq t,
                \\
                & \frac{h(x)}{\sum\limits_{y \colon T(y) = t} h(y)},\ \text{иначе}
            }
        \]

        \item[$1 \Ra 2$] Пусть $T(X)$ -- достаточная статистика, то есть условная вероятность не зависит от $\theta$:
        \[
            P_\theta(X = x | T(X) = t) = H(x, t)
        \]

        Тогда получим:
        \begin{multline*}
            f_\theta(x) = P_\theta(X = x) = P_\theta(X = x,\ T(X) = T(x)) =
            \\
            = P_\theta(T(X) = T(x)) \cdot P_\theta(X = x | T(X) = T(x)) =
            \\
            = P_\theta(T(X) = T(x)) H(x, T(x)) = \psi(T(x), \theta) h(x)
        \end{multline*}
    \end{itemize}
\end{proof}

\subsubsection{Улучшение оценок с помощью достаточных статистик}

\begin{theorem} (Колмогоров, Блэквелл, Рао, об улучшении несмещённой оценки)
    Пусть $T(X)$ -- достаточная статистика для $\theta$, $d(X)$ -- несмещённая оценка для $\tau(\theta)$. Тогда $\E_\theta (d(X) | T(X))$ зависит от выборки только через $T$ и не зависит от $\theta$, тогда обозначим $\phi(T(X)) = \E_\theta (d(X) | T(X))$ -- статистика, при этом:
    \begin{enumerate}
        \item $\E_\theta \phi(T(X)) = \tau(\theta)$
        \item $D_\theta \phi(T(X)) \le D_\theta d(X)$, здесь дисперсии могут быть бесконечными.
    \end{enumerate}

    Если $D_\theta d(X) < \infty$, то эквивалентны условия:
    \begin{enumerate}
        \item $D_\theta \phi(T(X)) = D_\theta d(X)$
        \item $\phi(T(X)) = d(X) \ P_\theta \text{-п.н.} \ \forall \theta$
        \item $d(X)$ является $T(X)$-измеримой
    \end{enumerate}
\end{theorem}

\begin{note}
    Из эквивалентных равенству условий равносильность $2 \Lra 3$ очевидна. Действительно, $\phi(T(X)) = d(X) \ P_\theta \text{-п.н.} \Lra \E_\theta (d(X) | T(X)) = d(X) \ P_\theta \text{-п.н.} \Lra$ $d(X)$ является $T(X)$-измеримой в силу определения и свойств условного матожидания.

    Для всего остального нам потребуется лемма.
\end{note}

\begin{lemma}
    Далее используются обозначения $L_1$ и $L_2$ для пространств Лебега измеримых функций.
    \begin{enumerate}
        \item $\eta \in L_1 \Ra \E(\E(\eta | \xi) - \E\eta)^2 \le D\eta$
        \item $\eta \in L_2 \Ra$ (Равенство достигается $\Lra \eta = \E(\eta | \xi) \Lra$ $\eta$ -- $\xi$-измеримая)
    \end{enumerate}
\end{lemma}

\begin{proof}
    Если $\eta \in L_1$, $\eta \not\in L_2$, то $D\eta = \infty$ и доказывать нечего. Далее считаем, что $\eta \in L_2$. Равносильность $\eta = \E(\eta | \xi) \Lra$ $\eta$ -- $\xi$-измеримая очевидна из свойств условного математического ожидания.

    Обозначим $\zeta = \E(\eta | \xi)$. Запомним, что $\zeta$ $\xi$-измерима. Тогда, применяя неравенство Йенсена для выпуклой вниз функции $h(t) = t^2$, опуская слова почти наверное, получим:
    \begin{align*}
        & \zeta^2 = (\E(\eta | \xi))^2 = h(\E(\eta | \xi)) \le \E(h(\eta) | \xi) = \E(\eta^2 | \xi)
        \\
        & \E \zeta^2 \le \E(\E(\eta^2 | \xi)) = \E \eta^2 
    \end{align*}

    Это нам нужно было только для того, чтобы понять, что $\zeta \in L_2$.
    
    Далее идея состоит в следующем: добавим и вычтем в разность внутри дисперсии $\eta$ случайную величину $\zeta$, которая является частичным матожиданием $\eta$, потом раскроем, как сумму квадратов, и выясним, что удвоенное произведение занулится.
    \[
        D\eta = \E(\eta - \E\eta)^2 = \E(\eta - \zeta + \zeta - \E\eta)^2 = \E(\eta - \zeta)^2 + \E(\zeta - \E\eta)^2 + 2 \E(\eta-\zeta)(\zeta-\E\eta)
    \]

    Заметим, что $\zeta-\E\eta$ $\xi$-измерима:
    \begin{multline*}
        \E(\eta-\zeta)(\zeta-\E\eta) = \E(\E[(\eta-\zeta)(\zeta-\E\eta) | \xi]) = \E((\zeta-\E\eta) \E(\eta-\zeta | \xi)) =
        \\
        = \E((\zeta-\E\eta) (\E(\eta | \xi) - \zeta)) = \E((\zeta-\E\eta) (\zeta - \zeta)) = 0
    \end{multline*}

    Получили, что $D\eta = \E(\eta - \zeta)^2 + \E(\zeta - \E\eta)^2$. Так как $\zeta = \E(\eta | \xi)$, то это мгновенно доказывает неравенство. Равенство достигается тогда и только тогда, когда $\E(\eta - \zeta)^2 = 0$, то есть $\eta = \zeta = \E(\eta | \xi)$.    
\end{proof}

\begin{proof}
    Теорема об улучшении несмещённой оценки. $T(X)$ -- достаточная статистика, то есть условное распределение $P_\theta(X \in B | T(X) = t)$ не зависит от $\theta$. Тогда условное распределение $P_\theta(d(X) \in B | T(X) = t) = P_\theta(X \in d^{-1}(B) | T(X) = t)$, равенство проверяется непосредственно по определению, тоже не зависит от $\theta$.

    Теперь, во-первых, условное матожидание является функцией от статистики $T(X)$, $\E_\theta(d(X) | T(X)) = \phi_\theta(T(X))$, что логично и следует из того, что условное матожидание $T(X)$-измеримо. Так как условное распределение $P_\theta(d(X) \in B | T(X) = t)$ не зависит от $\theta$, логично, что условное матожидание $\E_\theta(d(X) | T(X))$ не зависит от $\theta$, формально, кажется, это совсем быстро не обосновывается, надо переходить к связям через индикаторы, которые я не включал в напоминалку, и доказывать равенство интегралов. Савёлов тут помахал руками, так что, полагаю, можно забить. Итог: $\E_\theta(d(X) | T(X)) = \phi(T(X))$.

    Далее проверим несмещённость:
    \[
        \E_\theta d(X) = \tau(\theta) \Ra \E_\theta \phi(T(X)) = \E_\theta(\E_\theta(d(X) | T(X))) = \E_\theta d(X) = \tau(\theta)
    \]

    Оставшиеся пункты непосредственно следуют из леммы, если взять в качестве случайных величин $\eta = d(X)$, $\xi = T(X)$.
\end{proof}

\begin{definition}
    Наилучшую оценку $\tau(\theta)$ в классе несмещённых оценок в равномерном подходе с квадратичной функцией потерь называют оптимальной оценкой.
\end{definition}

\begin{note}
    В одномерном случае поиск оптимальной оценки сводится к сравнению дисперсий, на многомерный случай обобщается через сравнение матриц ковариаций с помощью неотрицательной определённости, подробно не будем на этом останавливаться.
\end{note}

\begin{definition}
    Статистика $S(X)$ называется полной для параметра $\theta$, если для любой измеримой функции $f$ выполнено следствие:
    \begin{align*}
        & \forall \theta \in \Theta \ \ \E_\theta f(S(X)) = 0
        \\
        & \Downarrow
        \\
        & \forall \theta \in \Theta \ \ f(S(X)) = 0 \ P_\theta \text{-п.н.} 
    \end{align*}
    Здесь важно, что квантор по $\theta$ встречается до и после следствия.
\end{definition}

\begin{note}
    Грубо термин полнота появился из следующих соображений: если всегда $\int_\cX f(S(X)) p_\theta(X) dX = 0$, рассматриваем случай доминируемого семейства распределений, то есть $f(S(X))$ ортогональна всем плотностям, то она всегда нулевая. Но аналогия здесь далеко не абсолютная.
\end{note}

\begin{theorem} (Лемана-Шеффера об оптимальной оценке)
    Пусть $T(X)$ -- полная достаточная статистика для семейства распределений $\{P_\theta,\ \theta \in \Theta\}$, $d(X)$ -- несмещённая оценка для $\tau(\theta)$. Тогда $\phi(T(X)) = \E_\theta(d(X) | T(X))$ -- несмещённая оценка с равномерно наименьшей дисперсией для $\tau(\theta)$, про оптимальность пока не говорим, так как дисперсия может быть бесконечной. Если $D_\theta \phi(T(X)) < \infty$, то $\phi(T(X))$ -- оптимальная оценка.
\end{theorem}

\begin{note}
    То есть теорема Колмогорова-Блэквелла-Рао даёт не просто улучшение оценки, но ещё и её оптимальность.
\end{note}

\begin{proof}
    Так как $\E_\theta \phi(T(X)) = \E_\theta (\E_\theta(d(X) | T(X))) = \E d(X) = \tau(\theta)$, то оценка является несмещённой. Если $\widetilde{d}(X)$ -- другая несмещённая оценка, то по теореме Колмогорова-Блэквелла-Рао $\widetilde{\phi}(T(X)) = \E_\theta(\widetilde{d}(X) | T(X))$ не хуже, а в случае конечной дисперсии строго лучше, оценки $\widetilde{d}(X)$. То есть если докажем, что $\phi(T(X)) = \widetilde{\phi}(T(X)) \ P_\theta \text{-п.н.} \ \forall \theta \in \Theta$, то докажем теорему.

    Обозначим $h(T(X)) = \phi(T(X)) - \widetilde{\phi}(T(X))$, хотим доказать, что $h(T(X)) = 0 \ P_\theta \text{-п.н.}$. Это является следствием полноты статистики $T(X)$, действительно:
    \begin{align*}
        & \forall \theta \in \Theta \ \ \E_\theta h(X) = \E_\theta \phi(T(X)) - \E_\theta \widetilde{\phi}(T(X)) = \tau(\theta) - \tau(\theta) = 0
        \\
        & \Downarrow
        \\
        & \forall \theta \in \Theta \ \ h(T(X)) = 0 \ P_\theta \text{-п.н.}
    \end{align*}
\end{proof}

\begin{corollary}
    Пусть $T(X)$ -- полная достаточная статистика для семейства распределений $\{P_\theta,\ \theta \in \Theta\}$, $\phi(T(X))$ -- несмещённая оценка для $\tau(\theta)$. Тогда $\phi(T(X))$ -- несмещённая оценка с равномерно наименьшей дисперсией для $\tau(\theta)$. Если $D_\theta \phi(T(X)) < \infty$, то $\phi(T(X))$ -- оптимальная оценка.
\end{corollary}

\begin{theorem} (Об экспоненциальном семействе, без доказательства)
    Пусть $X$ -- выборка из экспоненциального семейства распределений с обобщённой плотностью
    \[
        p_\theta(x) = h(x) \exp \ps{\sum_{i=1}^k a_i(\theta) T_i(x) + V(\theta)}
    \]

    Если область значений вектор-функции $(a_1(\theta), \dots, a_k(\theta))$ содержит $k$-мерный параллепипед в $\R^k$, то
    \[
        T(X) = \ps{\sum_{i=1}^n T_1(X_j), \dots, \sum_{i=1}^n T_k(X_j)}
    \]
    является полной достаточной статистикой для параметра $\theta$.
\end{theorem}

\begin{note}
    $k$-мерный параллелепипед можно заменить на открытое множество, так как одно всегда содержит второе и наоборот.
\end{note}

\begin{note}
    Алгоритм поиска оптимальной оценки:
    \begin{enumerate}
        \item Ищем достаточную статистику $T(X)$.
        \item Проверяем на полноту, если экспоненциальное семейство распределений, то всё хорошо, иначе, кажется, грустить и проверять по определению.
        \item Если полная, то решаем уравнение несмещённости, подбирая $g$:
        \[
            \E_\theta g(T(X)) = \tau(\theta) \ \ \forall \theta \in \Theta
        \]
        \item Если $D_\theta g(T(X)) < \infty$, то $g(T(X))$ -- оптимальная оценка.        
    \end{enumerate}
\end{note}

\begin{note}
    Причём, по-хорошему, для конечности дисперсии достаточно одного $\theta$, так как знаем, что при всех остальных $\theta$ оценка не хуже.
\end{note}